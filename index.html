<!DOCTYPE html>
<html lang="en">
	<head>
    	<meta charset="utf-8">
    	<title>Neural Algorithms</title>
    	<meta name="viewport" content="width=device-width, initial-scale=1.0">
    	<meta name="description" content="">
    	<meta name="author" content="">

    	<!-- Le styles -->
    	<link href="../personal_site/css/bootstrapFlatly.min.css" rel="stylesheet">
    	<style>
      	body {
        	/*padding-top: 60px;  60px to make the container go all the way to the bottom of the topbar */
      	}
    	</style>
    	<link href="../personal_site/css/bootstrap-responsive.min.css" rel="stylesheet">
    	<link href="../personal_site/css/main.css" rel="stylesheet">
	</head>
	<body>
	<div class="container">
	<div class="row-fluid">
	  <h3>Neural Algorithms Reading Group</h3>
   </div>
	<div class="row-fluid voffset1">
      	<span class="icon-time"></span><strong>Time:</strong> Fridays 11am-12:30pm, Spring Semester 2020 (first Meeting 2/14)
	</div>
	 <div class="row-fluid voffset1">
	  <span class="icon-map-marker"></span><strong>Location:</strong> Stata G451 (excpet on 3/6 on G825 and 4/10 on D507) <br>
	  
	  <div style="margin-top: 5px;">(Take G tower elevator to 4th floor and follow signs to room G451.)</div>
	</div>
	 <div class="row-fluid voffset1">
	  <span class="icon-envelope"></span><strong>Organizers:</strong> Nancy Lynch (lynch at csail dot mit dot edu), Mien Brabeeba Wang (brabeeba at mit dot edu), Lili Su (lilisu3 at csail dot mit dot edu)<br>
	</div>
	<a href="http://www.cameronmusco.com/neuralReadingGroup/">Last year's reading group website</a>
<div class="row-fluid voffset1">
	<div class="col-md-10 colp-lg-8" >
   		<img src="./triple.png" class="img-rounded img-responsive" alt="my photo">
</div>
  <div class="row voffset1">
    <div class="col-md-12">
      <section id="publications">
      	<h3>Background reading/watching</h3>
      	<ul>
      		<li>
      			<a href="https://b-ok.cc/book/2477222/4a05ed">Principles of Neural Science</a>
      		</li>
      		<li>
      			<a href="https://nancysbraintalks.mit.edu/">Nancy Kanwisher online course</a>
      		</li>
      		<li>
      			<a href="https://computationandbrain.github.io/about/">Christos Papadimitriou on-line course ("Computation and the Brain")</a>
      		</li>
      		<li>
      			<a href="http://cseweb.ucsd.edu/~dasgupta/254-neural-ul/index.html">Sanjoy Dasgupta’s course (“Neurally-inspired unsupervised learning")</a>
      		</li>
      		<li>
      			<a href="https://www.frontiersin.org/research-topics/39/spike-timing-dependent-plasticity">Reviews on Spike-timing dependent plasticity</a>
      		</li>
      	</ul>


        <h3>(Tentative) Schedule</h3>
		
		<p>(2/14) Overview and planning:  Nancy Lynch
			<ul>
			<li>Goals, overview of topics</li>
			<li>Planning topics, assigning papers/books to present</li>
			<li>Review of learning rules:
				<ul>
					<li> Short-term vs. long-term learning:  Setting up firing patterns vs. 
changing edge weights.</li>
<li> Local learning rules, vs. global optimization</li>
<li> Supervised and unsupervised learning</li>
<li>Hebbian learning in rate-based networks and SNNs, Oja, Other rules?  Hopfield network material?</li>
				</ul>

			</li>
			<strong>Source:</strong>
			<ul>
				<li><a href="https://page.mi.fu-berlin.de/rojas/neural/">Neural Networks---A Systematic Introduction</a>, Chapters 5, 13, 14.</li>
				<li><a href="./oja1992.pdf">Oja, Principal Components, Minor Components, and Linear Neural Networks</a></li>
				<li><a href="./oja1989">Oja, Neural Networks, Principal Components, and Subspaces</a> </li>
			</ul>
			<strong>What was presented:</strong>
			<ul>
				<li>Overview of brain algorithms.  </li>
				<li>Assignment of papers.</li>
				<li> <a href="./meeting1.pdf">Nancy's meeting note</a>
			
				</li>
			</ul>
		</ul>
		</p>
		
		<p>(2/21) Short-term learning:  Cameron Musco
			<ul>
				<strong>To read:</strong>
				<ul>
			<li><a href="https://drops.dagstuhl.de/opus/volltexte/2020/11708/pdf/LIPIcs-ITCS-2020-23.pdf">Hitron, Lynch, Musco, Parter. Random Sketching, Clustering, and Short-Term Memory in Spiking Neural Networks. ITCS, January 2020.</a>
			</li>
		</ul>
		<strong>What was presented:</strong>
		<ul>
			<li>Overview of the renaming paper by Cameron</li>
		</ul>
		
		</ul>
		</p>
		
		<p>(2/28) Oja's rule, analysis:  Brabeeba Wang and Chi-Ning Chou
			<ul>
				<strong>To read:</strong>
				<ul>
			<li> <a href="https://arxiv.org/pdf/1911.02363.pdf">Chou, Wang. ODE-Inspired Analysis for the Biological Version of Oja's Rule in Solving Streaming PCA with For-All-Time Guarantee. Submitted to COLT 2020</a>
			</li>
		</ul>
		<strong>What was presented:</strong>
		<ul>
			<li>Overview of the Oja's rule by Chi-Ning and Brabeeba</li>
			<li><a href="Theory and implementation of infomax filters for the retina.pdf">Haft and Hemmen. Theory and implementation of infomax filters for the retina. 1998.</a></li>
			<li>Biological plausibility and inplausibility of Oja's rule. <a href="cooper.pdf"></a>BCM rule review.</li>
			<li><a href="./oja1992.pdf">Oja, Principal Components, Minor Components, and Linear Neural Networks</a></li>
			<li><a href="https://arxiv.org/pdf/1503.00669.pdf">Cengiz Pehlevan, Tao Hu, Dmitri B. Chklovskii. A Hebbian/Anti-Hebbian Neural Network for Linear Subspace Learning: A Derivation from Multidimensional Scaling of Streaming Data. 2015</a></li>	
			
		</ul>
		
		</ul>
		</p>
		<p>(3/6) Coronavirus. Postpone meeting for a week and all the subsequent meetings are conducted over Zoom.
		</p>

		<p>(3/13) Valiant's work:  Nishanth Dikkala 
			<ul>
							<strong>To read:</strong>
							<ul>
								<li>Basic neuroid model and how it represents and learns concepts, for long-term learning.</li>
								<li>Limitations on memory capacity.</li>
								<li>Circuits of the Mind</li>
							</ul>
							<strong>What was presented:</strong>
							<ul>
								<li><a href="./neuroidal-model.pptx"></a>Slides</li>
								<li>Basic neuroid model and algorithms to solve memorization task</li>
								<li>Inductive learning will be spilled over to next week</li>
							</ul>
		
		</ul>

		</p>
		
		<p>(3/20) Valiant, cont'd:  Who?
			<ul>
				<strong>To read:</strong>
				<ul>
					<li>Circuits of the Mind. Continued.</li>
					<li>Valiant. Memorization and association on a realistic neural model. Neural Computation, 2006</li>
					
				</ul>
				
			</ul>
		</p>
		<p>(3/27) Valiant, cont'd.  Papadimitriou.  Who?
			<ul>
			<strong>To read:</strong>
			<ul>
				<li>Valiant. The Hippocampus as a Stable Memory Allocator for Cortex. Neural Computation, 2012</li>
				<li>Papadimitriou, Vempala. Cortical Learning via Prediction. JMLR 2015</li>
	
			</ul>
			
				</ul>
		</p>
		
			
		<p>(4/3)Papadimitriou papers:  Who?
			<ul>
			<strong>To read:</strong>
			<ul>
				<li>Maass, Papadimitriou, Vempala, Legenstein. Brain Computation:  A Computer Science Perspective.</li>
				<li>Papadimitriou, Vempala, Mitropolsky, Collins, Maass. Brain Computation by Assemblies of Neurons. BioRxiv.</li>
				<li>Legenstein, Maass, Papadimitriou, Vempala. Long Term Memory and the Densest K-Subgraph Problem. ITCS 2018</li>
			</ul>
			
		</ul>
		</p>
		<p>(4/10) Papadimitriou papers:  Who?
			<ul>
				<strong>To read:</strong>
				<ul>
				<li>Papadimitriou, Vempala. Random Projection in the Brain and Computation with Assemblies of Neurons. ITCS 2019</li> 
			</ul>
		</ul>
		</p>

		
		<p>(4/17) Other Papadimitriou work:  Who?
			<ul><strong>To read:</strong>
				<ul>
					<li>Pokorny, Ison, Rao, Legenstein Papa., Maass. STDP forms associations between memory traces in networks of spiking neurons</li>
					<li>Muller, Papa., Maass, Legenstein. A model for structured information representation in neural networks</li>
					<li>Papa. On the optimality of grid cells. ArXiv 2016</li>
					<li>Legenstein, Papa., Vampala, Maass. Assembly pointers for variable binding in networks of spiking neurons</li>
			</ul>
			
		</ul>
		</p>
		<p>(4/24) Connections with Artificial Neural Networks:  Lili Su
			<ul>
				<strong>To read:</strong>
				<ul>
					<li>Relating gradient descent algorithms to biological algorithms.</li>
				<li>Simulating gradient descent with local algorithms.</li>
				<li>Pehlevan work on similarity-based learning?  What else?</li>
				<li><a href="https://arxiv.org/pdf/1908.01867.pdf">Chklovskii's work: Neuroscience-inspired online unsupervised learning algorithms</a></li>			
			</ul>
			
		</ul>
		
		</p>
		<p>(5/1) Learning of logically structured concepts:  Nancy Lynch
			<ul>
				<strong>To read:</strong>
				<ul>
					<li>Lynch and Mallmann-Trenn. Learning Hierarchically Structured Concepts. To be submitted to COLT 2020.</li>
					<li>Mhaskar, Liao, Poggio. Learning functions:  When is deep better than shallow?. arXiv 2016</li>
					<li>Stern, Andreas, Klein.  A Minimum Span-Based Neural Constituency Parser.</li>
						<li>Zhou, Bau, Oliva, Torralba. Interpreting deep visual representations via network dissection. arXiv 2017. </li>
						<li>Hubel and Wiesel. Receptive fields of single neurons in the cat's striate cortex. J. Physiology 1959.</li>
						<li>Hubel and Wiesel. Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex. J. Physiology 1962.</li>
						<li>Felleman and van Essen. Distributed hierarchical processing in the primate cerebral cortex. Cerebral cortex, 1991.</li>
				</ul>
		
			</ul>
		</p>
		<p>(5/8)  Learning of concepts with geometric and/or temporal structure:  Brabeeba Wang, Chi-Ning-Chou
			<ul>
				<strong>To read:</strong>
				<ul>
					<li>Principles of Neural Science. Chapter 25, 26, 27.</li>
				</ul>
		
			</ul>
		
	<!--	<h4><strong>Unscheduled but need/want to fit in somewhere</strong></h4>
		<p>
			<ul>
				<li><a href="https://igi-web.tugraz.at/people/maass/psfiles/154.pdf">What Can a Neuron Learn with Spike-Timing-Dependent Plasticity?</a> Brabeeba might be interested in presenting.
			<li><a href="https://arxiv.org/abs/1803.09574">Long short-term memory and learning-to-learn in networks of spiking neurons</a>, Quanquan?</li>
			<li><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004347">Decreasing-Rate Pruning Optimizes the Construction of Efficient and Robust Distributed Networks</a>, Navlakha, Barth, Bar-Joseph.</li>
			<li>Other learning papers people are interested in?</li>
			</ul>
		</p>-->
	  </section>
	</div>
</div>
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../personal_site/js/jquery-1.11.1.min.js"></script>
    <script src="../personal_site/js/bootstrap.min.js"></script>

</div>
</body>
</html>